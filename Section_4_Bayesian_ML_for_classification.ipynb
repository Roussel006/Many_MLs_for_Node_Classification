{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc787fca-fa89-4e3b-88df-409f56ca6d92",
   "metadata": {},
   "source": [
    "# Experiments with Bayesian Machine Learning Models\n",
    "\n",
    "In this section, we build several multi-class classification models using Bayesian Machine Learning for the CiteSeer Dataset.\n",
    "\n",
    "1. Model 4a: A model with one-hot encoded vectors as the input features\n",
    "2. Model 4b: A model with Topics learned from Latent Semantic Analysis as the input features\n",
    "3. Model 4c: A model with Topics learned from Latent Dirichlet Allocation as the input features\n",
    "   \n",
    "<!-- 3. ???\n",
    "4. Model xyz: A model with Tensorflow embeddings for the vectors\n",
    "\n",
    "5. Model xyz: The effect of priors on prediction with the best model till now.\n",
    "6. Model xyz: Changing model architecture\n",
    " -->\n",
    "\n",
    "## Brief Intro to Bayesian Machine Learning\n",
    "\n",
    "Bayesian Machine Learning models are built on the principles of Bayesian statistics. In these models, we assume prior distributions of the parameters, representing our initial beliefs/knowledge about the parameters before seeing any data. As we train the model and observe data, we **update the priors using Bayes' theorem to obtain posterior distributions**. The posterior distributions represent our updated beliefs about the parameters after incorporating the data.\n",
    "\n",
    "A point to note is that Bayesian Machine Learning can be mathematically challenging due to the need to compute integrals over complex, high-dimensional probability distributions. These integrals are often intractable, making it difficult to directly calculate the posterior distributions of the model parameters. This is where Markov Chain Monte Carlo (MCMC) methods come to the rescue. MCMC algorithms, such as the No-U-Turn Sampler (NUTS) and Hamiltonian Monte Carlo (HMC), allow us to approximate these integrals by generating samples from the posterior distribution. Using these samples, we can estimate the posterior distributions and make inferences about the model parameters, effectively bypassing the need for exact integration -- the essence of any Monte Carlo method. This makes MCMC a powerful tool for Bayesian inference, enabling us to tackle complex problems that would otherwise be computationally prohibitive. We are going to use a popular implementation in Python -- PyMC -- to build our models.\n",
    "\n",
    "A key advantage of Bayesian ML is that it provides uncertainty estimates directly. That is, for any prediction, we can quantify the confidence in that prediction by looking at the spread of the posterior distribution. This is particularly useful in applications where understanding the uncertainty is crucial, such as in medical diagnosis, financial forecasting, autonomous driving, or modeling human behavior.\n",
    "\n",
    "In contrast, traditional Deep Learning models typically do not provide uncertainty estimates directly. These models are often deterministic, meaning that for a given input, they produce a single output without any measure of uncertainty. To estimate uncertainty in Deep Learning models, additional techniques are required, such as:\n",
    "\n",
    "1. Dropout: A regularization technique where random neurons are dropped during training. At inference time, dropout can be used to create an ensemble of models, and the variance in their predictions can be used as an uncertainty estimate.\n",
    "\n",
    "2. Ensemble Methods: Training multiple models independently and combining their predictions. The variance in the predictions of the ensemble members can be used to estimate uncertainty.\n",
    "\n",
    "While these methods can provide some measure of uncertainty, they are often less straightforward and may not capture the full range of uncertainty as effectively as Bayesian methods.\n",
    "\n",
    "In summary, Bayesian Machine Learning models offer a more natural and direct way to quantify uncertainty in predictions. This can be a significant advantage over other Machine Learning and Deep Learning methods in many applications.\n",
    "\n",
    "A big thanks for this section goes to Michael J. Schoelles for his awesome course on Cognitive Modeling at Rensselaer Polytechnic Institute. This course and a related course -- Programming for Cognitive Science and AI -- by Dr. Schoelles built the foundation of my Machine Learning knowledge and helped me immensely in developing this tutorial.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26d805-df97-4c4e-9fb7-5b5396ed5be0",
   "metadata": {},
   "source": [
    "## How to install PyMC?\n",
    "\n",
    "To me, the biggest challenge in doing Bayesian Machine Learning is setting up the packages correctly- specifically, the pymc/pymc3 packages and their many, many dependencies. It used to be very painful, but now things seem to have improved with the migration from \"pymc3\" to \"pymc\". Here, I will discuss the minimum steps I needed to get going in January 2025. Please see the [full installation guide here](https://www.pymc.io/projects/docs/en/latest/installation.html). Finally, I am using a Windows machine, but the installation seemed a bit easier on Linux machines.\n",
    "\n",
    "Basically, all we need to do is create and activate a virtual environment. Please DO NOT ignore the virtual env step. Things related to pymc3 broke my Python installation several times (yeah, several times, as I needed to be sure PyMC was breaking it. Sigh).\n",
    "\n",
    "We will be using **Conda** virtual environments. We will follow the first two steps from the install guide.\n",
    "\n",
    "       conda create -c conda-forge -n pymc_env \"pymc>=5\"\n",
    "       conda activate pymc_env\n",
    "\n",
    "Now, everything should be working nicely now! \n",
    "\n",
    "Of course, I tried other ways of doing it, but with limited success. The above method did seem to work the best. I am still getting the \"BLAS\" error mentioning C/Numpy and whatnot -- perhaps following the later optional parts from the install guide will help. But the speed of running the models seems good, and everything works, so I will address the error later to see if things improve more. Hit me up if you run into any problems or if you find a better solution.\n",
    "\n",
    "<u>OLD but keeping it in case handy</u>: Previously, we had to install some toolchain-related and other necessary packages. Seems not necessary to do now.\n",
    "\n",
    "       conda install mkl-service libpython m2w64-toolchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f5075-372f-48d4-b7ae-9260496f2f5d",
   "metadata": {},
   "source": [
    "## Get necessary things and create necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440e8f31-fb46-4934-b359-df7a406bf219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Functions to evaluate: accuracy, precision, recall, f1-score\n",
    "    # Many options. We will do it as we please, sometimes on our own\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,roc_curve\n",
    "\n",
    "# We need this one to avoid tensor issues in pymc. We will see how you can utilize this.\n",
    "import pytensor.tensor as pt\n",
    "import pymc as pm\n",
    "import arviz as az # Many things, one of which is saving the traces\n",
    "import pickle # To save models -- THIS FIRST CHOICE DID NOT WORK. So, using \"dill\" which works roughly the same way as pickle\n",
    "import dill as dpickle # To save models -- The second choice that worked.\n",
    "SAVE_DIR = \"Saved_BL_models_Exp4\"\n",
    "\n",
    "# For plotting and summarizing pymc things conveniently\n",
    "from arviz import plot_posterior, plot_trace, plot_pair, plot_kde, plot_forest\n",
    "from arviz import summary, hdi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d9c3cc-130a-4dc8-acc4-c43b3dcf8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four measures of model goodness together\n",
    "def calculate_results(y_true, y_pred):\n",
    "# Calculate model accuracy\n",
    "# Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \n",
    "# y_true: true labels in the form of a 1D array\n",
    "# y_pred: predicted labels in the form of a 1D array\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    # Calculate model precision, recall and f1 score using \"weighted\" average\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "    return(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503fcc62-70ff-4d5b-91fd-7b9cdb80e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The metrics here are calculated from the confusion matrix\n",
    "def accuracy(cm):\n",
    "    return np.trace(cm)/np.sum(cm)\n",
    "\n",
    "def error_rate(cm):\n",
    "    return (cm[0,1]+cm[1,0])/np.sum(cm)\n",
    "\n",
    "def precision(cm):\n",
    "    return cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "\n",
    "def recall(cm):\n",
    "    return cm[1,1]/(cm[1,1] + cm[1,0])\n",
    "\n",
    "def F1(cm):\n",
    "    tp,fp,fn = cm[1,1],cm[0,1],cm[1,0]\n",
    "    return tp/(tp + 1/2*(fp+fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a621930-f75d-490d-b69e-72c1e07a432b",
   "metadata": {},
   "source": [
    "## Get the Dataset\n",
    "\n",
    "We use the CiteSeer Node Classification task for our experiments. This dataset is one of the Planetoid datasets from the torch_geometric.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55dd926-e4c0-4995-ae7e-3c360b39d41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: CiteSeer()\n",
      "Input Text Data shape and type: torch.Size([3327, 3703]) torch.float32\n",
      "First five rows of the text data:\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Import dataset from PyTorch Geometric\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "dataset = Planetoid(root=\".\", name=\"CiteSeer\")\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "# Print information about the dataset\n",
    "print(\"Dataset name:\", dataset)\n",
    "print(\"Input Text Data shape and type:\", data.x.shape, data.x.dtype)\n",
    "print(\"First five rows of the text data:\\n\", data.x[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44925778-0eb9-4aa4-bf59-05eddebb3fc9",
   "metadata": {},
   "source": [
    "As we see, the dataset has 3327 documents as rows, made up of 3703 unique words. The documents are represented as one-hot vectors of length 3703. One hot vectors simply mean that if a word exists, then we assign it's magnitude to be 1 and if not, then we assign the magnitude to be 0. We just to need to follow the same order of words for each document, and that is it.\n",
    "\n",
    "An interesting point is the array type, which is \"torch.tensor\". Torch tensors are perfectly compatible with Numpy, so we should be fine.\n",
    "\n",
    "**Important**: Please note that we are not using all of the data available, rather using only about half of the documents. Moreover, we are using just 120 documents for training. The reason is that these are stipulations imposed in benchmarking different models that we saw earlier. We keep the split as is to be able to compare with the state-of-the-art results.\n",
    "\n",
    "Now, we are ready to get modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7eb8db-3179-4fc8-b6dc-084286a07230",
   "metadata": {},
   "source": [
    "### Train-Validation-Test Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5208b8e-1040-42d3-b93d-0c4cb75b274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 3703]) torch.Size([500, 3703]) torch.Size([1000, 3703])\n"
     ]
    }
   ],
   "source": [
    "train_sentences_one_hot = data.x[data.train_mask]\n",
    "train_labels = data.y[data.train_mask]\n",
    "\n",
    "val_sentences_one_hot = data.x[data.val_mask]\n",
    "val_labels = data.y[data.val_mask]\n",
    "\n",
    "test_sentences_one_hot = data.x[data.test_mask]\n",
    "test_labels = data.y[data.test_mask]\n",
    "\n",
    "print(train_sentences_one_hot.shape, val_sentences_one_hot.shape, test_sentences_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde510ad-cf65-447e-b615-b95778c7c0e8",
   "metadata": {},
   "source": [
    "# Model Set 4: Bayesian Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a9ade-bbb9-4f73-83e7-4a7f68cd1743",
   "metadata": {},
   "source": [
    "## Model 4a: Using all 3703 words directly as features for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9daf393b-a4ba-422c-9629-3797fb4f46eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = np.unique(data.y).shape[0]\n",
    "num_features = data.x.shape[1]\n",
    "\n",
    "np.array(train_sentences_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4109e14a-1af7-4477-9f7d-90a1b1fcad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "# with pm.Model() as model_4a:\n",
    "#     alpha = pm.Normal('alpha', mu=0, sigma=2, shape= num_classes)\n",
    "#     beta = pm.Normal('beta', mu=0, sigma=2, shape=(num_features, num_classes))\n",
    "\n",
    "#     mu = alpha + pm.math.dot(np.array(train_sentences_one_hot), beta) # The observed input here!\n",
    "#     # theta = tt.nnet.softmax(mu)\n",
    "#     theta = pt.special.softmax(mu, axis = 1) # Softmax over the mu values\n",
    "#     yl = pm.Categorical('yl', p=theta, observed = train_labels) # observed train_labels here\n",
    "#     trace_3a = pm.sample(2000) \n",
    "# plot_trace(trace_3a)\n",
    "# summary(trace_3a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92109a16-f51a-44cd-9ecd-1c2ce8aee344",
   "metadata": {},
   "source": [
    "### What did we learn from this experiment?\n",
    "The number of features is a big challenge for us. The processes taking too long -- one sampling process did not end in three hours. To have a chance at completing the computations on our personal computers, we need to find a way to reduce the complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0daba2-04ce-4dbd-a254-3d86496178f8",
   "metadata": {},
   "source": [
    "## Model 4b: LSA Topic Model + PyMC model\n",
    "\n",
    "We will use a \"Topic modeling\" approach as a form of feature engineering. Specifically, we will reduce the many words down to some topics that explain most of our data. This way, these topics would be our features, reducing the number of features.\n",
    "\n",
    "First, we will use a Latent Semantic Analysis or Indexing (LSA/LSI), a popular topic modeling approach. LSA combines a process of TF-IDF vectorization (discussed in Section 1) with a Singular Value Decomposition (SVD) with truncation (simply, Truncated SVD). It is the SVD step that gives the topics. The SVD process decomposes or factorizes our document-word matrix (A) into three factor matrices: A document-topic matrix ($U$), a diagonal matrix containing the singular values ($S$), and a topic-word matrix ($V^T$). \n",
    "\n",
    "\\begin{equation}\n",
    "A = U*S*V^T\n",
    "\\end{equation}\n",
    "\n",
    "Essentially, we are transforming our original data vectors into a latent space of eigenvectors. These eigenvectors represent our topics of interest. The Singular values -- which are the square roots of the eigenvalues corresponding to the eigenvectors -- indicate how useful the eigenvectors are in explaining the texts. When we truncate, we keep only the highest n number of singular values and their corresponding eigenvectors and ignore the rest. In a way, we look for a more compact representation of our data in latent space using a only few of the eigenvalues learned through SVD.\n",
    "\n",
    "**Why not PCA?** Another choice is using Principal Component Analysis or PCA, but it works better with dense data. For sparse data like the word vectors we have, SVD is more appropriate. Notably, PCA is a special case of SVD when it is performed on the covariance matrix. SVD applies to any matrix, especially when centering and calculating variance based on that does not make much sense.\n",
    "\n",
    "Another option we will use later is Latent Dirichlet Allocation, another topic modeling technique which is based on the Bayes theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b157d37c-e4f7-4475-aeee-0240ede1b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Vector representations and transformations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "# Classifiers assemble!\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f7f18-c655-4afd-92a8-0741d87142c0",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3adbadd1-3427-414b-91b3-8bfaee22ebf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;svd&#x27;, TruncatedSVD(n_components=120))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;svd&#x27;, TruncatedSVD(n_components=120))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\">?<span>Documentation for TfidfTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfTransformer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TruncatedSVD<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.TruncatedSVD.html\">?<span>Documentation for TruncatedSVD</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TruncatedSVD(n_components=120)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfTransformer()),\n",
       "                ('svd', TruncatedSVD(n_components=120))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do LSA!\n",
    "num_topics = 120 # Max_topics = min (total number of features, total number of documents for training)\n",
    "lsa_topic_model_4b = Pipeline([\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    (\"svd\", TruncatedSVD(n_components = num_topics))])\n",
    "lsa_topic_model_4b.fit(train_sentences_one_hot, train_labels)\n",
    "\n",
    "# lsa_topic_model_4b.transform(train_sentences_one_hot)\n",
    "\n",
    "# print(calculate_results(y_pred = model_4b.predict(train_sentences_one_hot), y_true = train_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a822d07-d0bd-493d-91f1-99222959955a",
   "metadata": {},
   "source": [
    "### PyMC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11bef83-0a95-43a4-b612-254a654ab10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha, beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f156a09f6ec24c659bfe42e19aa62fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 29 seconds.\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.unique(data.y).shape[0]\n",
    "num_features = num_topics ## An important change!\n",
    "\n",
    "# Input data\n",
    "\n",
    "train_X = lsa_topic_model_4b.transform(train_sentences_one_hot)\n",
    "train_y = train_labels\n",
    "\n",
    "# Define and fit model.\n",
    "with pm.Model() as bayes_model_4b:\n",
    "    X = pm.Data(\"X\", train_X)\n",
    "    y = pm.Data(\"y\", train_y)\n",
    "    \n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=2, shape= num_classes) # The intercepts of linear regression\n",
    "    beta = pm.Normal('beta', mu=0, sigma=2, shape=(num_features, num_classes)) # The coefficients\n",
    "\n",
    "    mu = alpha + pm.math.dot(X, beta) # The observed input here!\n",
    "        # NOTE: We wrapped our input data with a name. \n",
    "        # This one is needed for predicting on new data (validation, testing)\n",
    "        # Without it, the line would look like this:\n",
    "    # mu = alpha + pm.math.dot(train_X, beta) # The observed input here!\n",
    "    \n",
    "    theta = pt.special.softmax(mu, axis = 1) # Softmax probability over the mu values\n",
    "\n",
    "    yl = pm.Categorical('yl', p=theta, observed = y, shape = X.shape[0]) # observed train_labels here\n",
    "\n",
    "    trace_4b = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86a7f41a-2425-4a50-98c0-7858f8653ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>0.058</td>\n",
       "      <td>0.947</td>\n",
       "      <td>-1.734</td>\n",
       "      <td>1.790</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>3129.0</td>\n",
       "      <td>4777.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[1]</th>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-1.722</td>\n",
       "      <td>1.762</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>3072.0</td>\n",
       "      <td>4700.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[2]</th>\n",
       "      <td>0.053</td>\n",
       "      <td>0.943</td>\n",
       "      <td>-1.874</td>\n",
       "      <td>1.746</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>3039.0</td>\n",
       "      <td>4723.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[3]</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.959</td>\n",
       "      <td>-1.787</td>\n",
       "      <td>1.813</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>3184.0</td>\n",
       "      <td>4707.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[4]</th>\n",
       "      <td>0.018</td>\n",
       "      <td>0.963</td>\n",
       "      <td>-1.827</td>\n",
       "      <td>1.794</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>3116.0</td>\n",
       "      <td>4867.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[119, 1]</th>\n",
       "      <td>-0.088</td>\n",
       "      <td>1.968</td>\n",
       "      <td>-3.818</td>\n",
       "      <td>3.586</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.027</td>\n",
       "      <td>14980.0</td>\n",
       "      <td>5353.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[119, 2]</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>1.951</td>\n",
       "      <td>-3.589</td>\n",
       "      <td>3.841</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.026</td>\n",
       "      <td>13906.0</td>\n",
       "      <td>5448.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[119, 3]</th>\n",
       "      <td>0.014</td>\n",
       "      <td>1.942</td>\n",
       "      <td>-3.482</td>\n",
       "      <td>3.731</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.028</td>\n",
       "      <td>14625.0</td>\n",
       "      <td>5298.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[119, 4]</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>1.919</td>\n",
       "      <td>-3.542</td>\n",
       "      <td>3.647</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>12322.0</td>\n",
       "      <td>5971.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[119, 5]</th>\n",
       "      <td>0.163</td>\n",
       "      <td>1.849</td>\n",
       "      <td>-3.290</td>\n",
       "      <td>3.583</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.024</td>\n",
       "      <td>15813.0</td>\n",
       "      <td>5517.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>726 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "alpha[0]      0.058  0.947  -1.734    1.790      0.017    0.012    3129.0   \n",
       "alpha[1]     -0.025  0.940  -1.722    1.762      0.017    0.012    3072.0   \n",
       "alpha[2]      0.053  0.943  -1.874    1.746      0.017    0.012    3039.0   \n",
       "alpha[3]     -0.006  0.959  -1.787    1.813      0.017    0.012    3184.0   \n",
       "alpha[4]      0.018  0.963  -1.827    1.794      0.017    0.012    3116.0   \n",
       "...             ...    ...     ...      ...        ...      ...       ...   \n",
       "beta[119, 1] -0.088  1.968  -3.818    3.586      0.016    0.027   14980.0   \n",
       "beta[119, 2] -0.006  1.951  -3.589    3.841      0.017    0.026   13906.0   \n",
       "beta[119, 3]  0.014  1.942  -3.482    3.731      0.016    0.028   14625.0   \n",
       "beta[119, 4] -0.012  1.919  -3.542    3.647      0.017    0.024   12322.0   \n",
       "beta[119, 5]  0.163  1.849  -3.290    3.583      0.015    0.024   15813.0   \n",
       "\n",
       "              ess_tail  r_hat  \n",
       "alpha[0]        4777.0    1.0  \n",
       "alpha[1]        4700.0    1.0  \n",
       "alpha[2]        4723.0    1.0  \n",
       "alpha[3]        4707.0    1.0  \n",
       "alpha[4]        4867.0    1.0  \n",
       "...                ...    ...  \n",
       "beta[119, 1]    5353.0    1.0  \n",
       "beta[119, 2]    5448.0    1.0  \n",
       "beta[119, 3]    5298.0    1.0  \n",
       "beta[119, 4]    5971.0    1.0  \n",
       "beta[119, 5]    5517.0    1.0  \n",
       "\n",
       "[726 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot_trace(trace_4b)\n",
    "summary(trace_4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a74d8-d6f4-4bc0-8291-1d9d20638286",
   "metadata": {},
   "source": [
    "### Predict using the Posterior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e7dece3-ecb3-49e6-a18d-28d1aeca29f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [yl]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba7c4162a614916bff61a5a719344fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Posterior predictive sampling \n",
    "posterior_predictive_4b = pm.sample_posterior_predictive(trace_4b, predictions = 1, model = bayes_model_4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba2f063c-c767-45c9-8fba-3a739cf9260a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2000, 120)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior_predictive_4b[\"predictions\"][\"yl\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd42e0-7586-4918-8d20-5590bad6b620",
   "metadata": {},
   "source": [
    "**Notes about traces and the sample_posterior_predictive**:\n",
    "We ran four chains, and in each chain, we sampled 2000 times. For each of $4\\times2000 = 8000$ samples, we have a prediction. Let us see how the prediction accuracy changed with learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923374f-2a11-4b26-8a88-9b25bf3bb05a",
   "metadata": {},
   "source": [
    "### Evaluating Accuracy\n",
    "\n",
    "First, let's check with a single simulation or a set of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3cf64fb-a042-4560-bd41-9d526bc68f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 51.66666666666667,\n",
       " 'precision': 0.5467220602153137,\n",
       " 'recall': 0.5166666666666667,\n",
       " 'f1': 0.5207835123598934}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = posterior_predictive_4b[\"predictions\"][\"yl\"][3][0]\n",
    "y_true = train_labels\n",
    "calculate_results(y_pred = y_pred, y_true = y_true) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c7230-ab97-48df-bc66-7c460fbd956e",
   "metadata": {},
   "source": [
    "The above function can do only one comparison at a time, although gives us all four measures together. Let's break them up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f22101f-f231-40ac-85be-452b1d461431",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all = posterior_predictive_4b[\"predictions\"][\"yl\"]\n",
    "\n",
    "# The first chain only\n",
    "cm = [confusion_matrix(y_true = train_labels, y_pred = y_pred_all[0][i]) for i in range(y_pred_all[0].shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10ae174a-9d6c-47fe-a5e7-b350e6f9a2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.513808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.046658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.691667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          accuracy\n",
       "count  2000.000000\n",
       "mean      0.513808\n",
       "std       0.046658\n",
       "min       0.350000\n",
       "25%       0.483333\n",
       "50%       0.516667\n",
       "75%       0.541667\n",
       "max       0.691667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ = [accuracy(item) for item in cm]\n",
    "pd.DataFrame({'accuracy':accuracy_}).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e5c14-edfe-4515-9b6c-597015a50358",
   "metadata": {},
   "source": [
    "**Notes on Accuracy**: As we see, the average accuracy is about 50% which is three times better than random. At best, we got about 65% correct; at worst, we got about 33% correct. However, the training accuracy is much lower than any of the models we built so far and close to the test accuracies of the Shallow ML methods (Section 1) and Deep Learning models (Section 2), but worse than the GNNs (Section 3) that used additional information of the edges.\n",
    "\n",
    "1. The role of priors. If our priors are too far off the real distributions, we need a lot of data for the posterior distributions to closely resemble the real ones. As we used very little data, we need better priors to improve model accuracy.\n",
    "\n",
    "2. Words and Topics as Features:\n",
    "   Categorical features (such as words and topics) are not very common in Bayesian ML in my experience.\n",
    "   Moreover, topics as features was not very useful before either in shallow models. In fact, they performed better on the raw vectors. Here, we could not use all of the features. We will see how it goes with the LDA approach in the next section.\n",
    "\n",
    "3. Different model architectures may help improve the performance.\n",
    "\n",
    "Another possibility: Here, we only checked the accuracy of the training. Let's see how we do on the validation and test accuracies. If the test accuracy is close to the training accuracy, then our model is actually great and was not overfitted as the models in Sections 1 and 2 were. But, as we will see, the test accuracy is even lower, showing our model is not performing so well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dbfaba2-80a9-4104-b03b-b1827e9b38f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [yl]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de98969cb1a24d9780685c69880cb8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_X = lsa_topic_model_4b.transform(test_sentences_one_hot)\n",
    "with bayes_model_4b:\n",
    "    # Set the unseen test data to replace the training data\n",
    "    pm.set_data({'X': test_X}) \n",
    "    # Predict response data for new data\n",
    "    posterior_predictive_4b_test = pm.sample_posterior_predictive(trace_4b, predictions = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33b22f04-e754-4ede-a2d0-9fa376a23bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.240784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.016586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.302000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          accuracy\n",
       "count  2000.000000\n",
       "mean      0.240784\n",
       "std       0.016586\n",
       "min       0.186000\n",
       "25%       0.230000\n",
       "50%       0.241000\n",
       "75%       0.252000\n",
       "max       0.302000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_all = posterior_predictive_4b_test[\"predictions\"][\"yl\"]\n",
    "y_true_all = test_labels\n",
    "\n",
    "# The first chain only\n",
    "cm = [confusion_matrix(y_true = y_true_all, y_pred = y_pred_all[0][i]) for i in range(y_pred_all[0].shape[0])]\n",
    "accuracy_ = [accuracy(item) for item in cm]\n",
    "pd.DataFrame({'accuracy':accuracy_}).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f25c25-b235-4363-878d-30bebb592490",
   "metadata": {},
   "source": [
    "As we see test accuracy is quite poor and much lower than train accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb72f5-8847-492b-b434-780fb52de3b2",
   "metadata": {},
   "source": [
    "### Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a95f0943-c010-4c4b-ba43-9ee2f600c2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saved_BL_models_Exp4/trace_model_4b.nc'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to save both the model and the trace.\n",
    "# Save the model\n",
    "with open(SAVE_DIR + 'bayes_model_4b.pkl', 'wb') as file_to_write:\n",
    "    dpickle.dump(bayes_model_4b, file_to_write)\n",
    "                                                    \n",
    "# Save the trace\n",
    "az.to_netcdf(trace_4b, SAVE_DIR + '/trace_model_4b.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9eff695-d97b-484e-98d9-660e4f9e676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [yl]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dc9a68b1254bed8c281a7f91adeb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "with open(SAVE_DIR + 'bayes_model_4b.pkl', 'rb') as file_to_read:\n",
    "    loaded_bayes_model_4b = dpickle.load(file_to_read)\n",
    "\n",
    "# Load the trace \n",
    "loaded_trace = az.from_netcdf('Saved_BL_models_Exp4/trace_model_4b.nc') \n",
    "# Use the loaded trace for posterior predictive sampling \n",
    "with loaded_bayes_model_4b:\n",
    "    posterior_predictive_new = pm.sample_posterior_predictive(loaded_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790e414-74b2-4ffc-a177-8b76740a5ed4",
   "metadata": {},
   "source": [
    "## Model 4c: LDA Topic Model + PyMC model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca02d76-17ad-43a1-aea5-2c08afbb3949",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9189807-60df-475b-b15d-20c27665aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "num_topics = 100 # Need to be less than the total number of features\n",
    "\n",
    "lda_topic_model_4c = LatentDirichletAllocation(n_components = num_topics, random_state = 2)  # Adjust the number of components as needed\n",
    "# train_sentences_reduced = lda_topic_model_4c.fit_transform(train_sentences_one_hot)\n",
    "# print(train_sentences_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa017e-e33d-4f58-8214-498f31c3d616",
   "metadata": {},
   "source": [
    "### PyMC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13970cef-9166-4dbc-84ea-b128b49b318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha, beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582421b402534c97a7ef74192ad4e086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 29 seconds.\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.unique(data.y).shape[0]\n",
    "num_features = num_topics ## An important change!\n",
    "\n",
    "# Input data\n",
    "\n",
    "train_X = lda_topic_model_4c.fit_transform(train_sentences_one_hot)\n",
    "train_y = train_labels\n",
    "\n",
    "# Define and fit model.\n",
    "with pm.Model() as bayes_model_4c:\n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=2, shape= num_classes)\n",
    "    beta = pm.Normal('beta', mu=0, sigma=2, shape=(num_features, num_classes))\n",
    "\n",
    "    mu = alpha + pm.math.dot(train_X, beta) # The observed input here!\n",
    "    # theta = tt.nnet.softmax(mu)\n",
    "    theta = pt.special.softmax(mu, axis = 1) # Softmax over the mu values\n",
    "    yl = pm.Categorical('yl', p=theta, observed = train_y) # observed train_labels here\n",
    "    trace_4c = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b32d2-94ad-45f3-884c-6de6c17d7413",
   "metadata": {},
   "source": [
    "### Predict using the Posterior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "705ba453-1c16-4948-834c-d9973b684bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [yl]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d434fa487c954c9189493c74d8f5b1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Posterior predictive sampling \n",
    "posterior_predictive_4c = pm.sample_posterior_predictive(trace_4c, predictions = 1, model = bayes_model_4c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75d2f8-7204-4fd5-8cae-64698887b988",
   "metadata": {},
   "source": [
    "### Evaluating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e4bc521-13a8-405e-aa50-fac455b633eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.384958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.042156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.358333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.383333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          accuracy\n",
       "count  2000.000000\n",
       "mean      0.384958\n",
       "std       0.042156\n",
       "min       0.250000\n",
       "25%       0.358333\n",
       "50%       0.383333\n",
       "75%       0.416667\n",
       "max       0.533333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_all = posterior_predictive_4c[\"predictions\"][\"yl\"]\n",
    "\n",
    "# The first chain only\n",
    "cm = [confusion_matrix(y_true = train_labels, y_pred = y_pred_all[0][i]) for i in range(y_pred_all[0].shape[0])]\n",
    "\n",
    "accuracy_ = [accuracy(item) for item in cm]\n",
    "pd.DataFrame({'accuracy':accuracy_}).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fe670-81da-4c28-aeed-356b6a00fdce",
   "metadata": {},
   "source": [
    "As we can see, using LDA instead of LSA has worsened performance to some extent. The accuracy is down to 38% from 50%. Similar reductions can be observed in the minimum and maximum accuracies. In the future, we will do more experiments using different combinations of models, such as using graph embeddings (from Section 3) as input for the Bayesian models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
