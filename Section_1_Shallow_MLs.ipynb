{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Experiments with \"Shallow\" ML algorithms\n",
    "\n",
    "In this section, we will build eight \"shallow\" ML classifiers for text/node classification. We will learn how to implement each of them using Scikit-Learn and save the models for later comparisons with other models. The models we will develop are:\n",
    "\n",
    "    1a. Naive Bayes\n",
    "    1b. XGBoost\n",
    "    1c. Decision Trees\n",
    "    1d. Random Forest\n",
    "    1e. Gradient Boosting\n",
    "    1f. CatBoost\n",
    "    1g. LightGBM\n",
    "    1h. Support Vector Machine (SVM) Classifiers\n",
    "\n",
    "\n",
    "**The Dataset** we will use is the CiteSeer Dataset and classify the documents or the nodes. This dataset is a popular benchmark for Graph-based MLs. As of January 2025, the best accuracy achieved is **82.07 Â± 1.04** by [\"ACMII-Snowball-2\"](https://paperswithcode.com/paper/is-heterophily-a-real-nightmare-for-graph). A live update on the rankings can be found in this [link](https://paperswithcode.com/sota/node-classification-on-citeseer).\n",
    "\n",
    "Can we beat it? Perhaps not so easily, as brilliant ML scientists and engineers have already thrown the kitchen sink at it. But we can definitely try! Why not dream? We will see how close we can get.\n",
    "\n",
    "**The General Plan**:\n",
    "1. <u>Build a Modeling Pipeline</u>: For each model, we will create a \"pipeline\". The pipelines can include everything between inputs and outputs. For example, we may want to represent our texts as certain kind of vectors (e.g., one_hot, TF-IDF). Then, We may want to transform our vectors and reduce their dimensions using methods such as Singular Value Decomposition (SVD) or Non-negative Matrix Factorization (NMF). Finally, we would have our model to feed these all into. This workflow can be conveniently represented as a pipeline, as we will see.\n",
    "\n",
    "2. <u>Train, Validate, and Test</u>: After training, we will check the validation and the test accuracies. \n",
    "\n",
    "3. <u>Save the Models</u>: We will then save the models so that we can call them up again in later sections.\n",
    "\n",
    "It is almost as simple as it sounds. Of course, there are some nuances. But, we do not need to worry too much about it now. We will discuss things as they become necessary.\n",
    "\n",
    "Enough talking! Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing, get some essential Packages\n",
    "\n",
    "# Numpy for matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "\n",
    "# Visualization\n",
    "import networkx as nx\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the CiteSeer Dataset\n",
    "This dataset is available through PyTorch Geometric, a package dedicated to Graph NNs. The CiteSeer is one of the several datasets available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# Import dataset from PyTorch Geometric\n",
    "dataset = Planetoid(root=\".\", name=\"CiteSeer\")\n",
    "\n",
    "data = dataset[0] # We extract the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: CiteSeer()\n",
      "Input Text Data shape: torch.Size([3327, 3703])\n",
      "First five rows of the text data:\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Print information about the dataset\n",
    "print(\"Dataset name:\", dataset)\n",
    "print(\"Input Text Data shape:\", data.x.shape)\n",
    "print(\"First five rows of the text data:\\n\", data.x[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the dataset has 3327 documents as rows, made up of 3703 unique words. The documents are represented as one-hot vectors of length 3703. One hot vectors simply mean that if a word exists, then we assign it's magnitude to be 1 and if not, then we assign the magnitude to be 0. We just to need to follow the same order of words for each document, and that is it.\n",
    "\n",
    "An interesting point is the array type, which is \"torch.tensor\". Torch tensors are perfectly compatible with Numpy, so we should be fine.\n",
    "\n",
    "Now, we are ready to get modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Set 1: Shallow Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
