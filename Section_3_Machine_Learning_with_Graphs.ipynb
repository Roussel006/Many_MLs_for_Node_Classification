{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Experiments with Graph-based Learning\n",
    "\n",
    "In this section, we will build several Graph-based MLs. There are several types of ML with Graphs -- Feature Engineering (for Graph Characteristics as features), Graph Representation Learning, Graph Neural Networks, and more. Here, we will implement and compare some of them.\n",
    "\n",
    "    3a. A Graph Convolutional Network (GCN)\n",
    "    3b. A Graph Attention Network (GAT)\n",
    "\n",
    "\n",
    "**The Dataset** we will use is the CiteSeer Dataset and classify the documents or the nodes. This dataset is a popular benchmark for Graph-based MLs. As of January 2025, the best accuracy achieved is **82.07 Â± 1.04** by [\"ACMII-Snowball-2\"](https://paperswithcode.com/paper/is-heterophily-a-real-nightmare-for-graph). A live update on the rankings can be found in this [link](https://paperswithcode.com/sota/node-classification-on-citeseer).\n",
    "\n",
    "Can we beat it? Perhaps not so easily, as brilliant ML scientists and engineers have already thrown the kitchen sink at it. But we can definitely try! Why not dream? We will see how close we can get.\n",
    "\n",
    "The information within the dataset: This dataset contains a set of 3327 scientific papers represented by binary vectors of 3703 words, with the values represent the presence or absence of the words in the document. A **key feature** of the dataset is that it also contains data on the citations among the papers as a citation graph or network, along with the text data. Here we are only use the text data. In later sections, we will incorporate the Graph data and see how it changes things. The availability of both types of data is the biggest reason we picked this dataset.\n",
    "\n",
    "<!-- **The General Plan**: -->\n",
    "<!-- 1. <u>Build a Modeling Pipeline</u>: For each model, we will create a \"pipeline\". The pipelines can include everything between inputs and outputs. For example, we may want to represent our texts as certain kind of vectors (e.g., one_hot, TF-IDF). Then, We may want to transform our vectors and reduce their dimensions using methods such as Singular Value Decomposition (SVD) or Non-negative Matrix Factorization (NMF). Finally, we would have our model to feed these all into. This workflow can be conveniently represented as a pipeline, as we will see.\n",
    "\n",
    "2. <u>Train, Validate, and Test</u>: After training, we will check the validation and the test accuracies. \n",
    "\n",
    "3. <u>Save the Models</u>: We will then save the models so that we can call them up again in later sections.\n",
    "\n",
    "It is almost as simple as it sounds. Of course, there are some nuances to these methods. But, we do not need to worry too much about it now. We will discuss things as they become necessary.\n",
    "\n",
    "Enough talking! Let's get started! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Intro to Machine Learning with Graphs\n",
    "Learning from graphs is an age-old approach (tracing back to Leonhard Euler!) that has seen a (relatively) recent resurgence for applications in Deep Learning. We briefly discuss the different types and how they relate to traditional Machine Learning. For details, a great open-source resource for ML with Graphs is the CS224W course by Jure Lescovec (https://web.stanford.edu/class/cs224w/). The lecture slides, notebooks, and problem sets for several years are provided in entirety, such as the Fall 2023 resource can be found here: https://snap.stanford.edu/class/cs224w-2023/. Lecture videos for particular years can be found on YouTube. Here is the lecture series for 2021: https://youtu.be/JAB_plj2rbA?si=ujCiFl6HBFSqlGVf\n",
    "\n",
    "**What is a Graph?**\n",
    "\n",
    "A graph or a network can be considered as set of variables as **nodes** or **vertices** that are connected by some relationships represented by **edges**. Graphs are generally denoted by $G = (V, E, W)$ where V is a set of vertices or nodes, E is the set of edges connecting them, and W represent the edge weights or strengths of relationship between nodes.\n",
    "\n",
    "The graphs are **undirected** if the edges have no direction or **directed** if they do. If the edge weights are binary (1 if a connection exists, 0 if not), the graphs are called **unweighted** -- somewhat paradoxically, I must admit -- as the weights simply mean the existence of connections and with no information on how strong are the relationships. When the weights are non-binary and represent, the graphs are **weighted graphs**.\n",
    "\n",
    "The graph in the CiteSeer Dataset is an **undirected, unweighted** graph, in essence, the simplest kind. As we will see, the incorporation of the edge information after setting each document as a node, will help us exceed the accuracies we had achieved with the shallow and the deep networks.\n",
    "\n",
    "### Different types of Learning from Graphs\n",
    "\n",
    "**Feature Engineering to encode information from graphs**\n",
    "\n",
    "In this case, we develop new features that capture the information contained within the graphs and use it in a traditional machine learning pipeline.\n",
    "\n",
    "    * Manual Feature Extraction: Extracting features like node degree, clustering coefficient, and centrality measures.\n",
    "    * Graph Kernels: Methods like Weisfeiler-Lehman Kernel and Shortest Path Kernel that measure similarity between graphs.\n",
    "    * Graph Augmentation: Techniques that enhance graph data by adding or modifying nodes and edges.\n",
    "\n",
    "**Graph Representation Learning**\n",
    "\n",
    "    1. Node Embeddings: Techniques like DeepWalk or node2vec that learn low-dimensional representations of nodes.\n",
    "    2. Edge Embeddings: Techniques that focus on learning representations for edges in a graph, perhaps for a link or edge prediction task.\n",
    "    3. Graph Embeddings: Methods like GraphSAGE and Graph Convolutional Networks (GCNs) that learn representations for entire graphs.\n",
    "\n",
    "The representations can also be learned from the embedding layer of the neural networks trained for a downstream task. For example, the node embeddings can be learned from an RNN or a GCN trained to predict the labels. Therefore, in essence, any of the following approaches can be used for graph representation learning. Moreover, the learned representation themselves can be reused for other downstream tasks, such as community detection, clustering, and even to train a supervised/semi-supervised classification model using the embeddings from the already trained model. We will explore this approach in one future experiment.\n",
    "\n",
    "**Graph Neural Networks (GNNs)**\n",
    "    \n",
    "    * Graph Convolutional Networks (GCNs): Neural networks that apply convolution operations on graphs.\n",
    "    * Graph Attention Networks (GATs): GNNs that use attention mechanisms to weigh the importance of neighboring nodes.\n",
    "    * Graph Recurrent Networks (GRNs): GNNs that incorporate recurrent neural network architectures for sequential data.\n",
    "    * Graph Autoencoders: Unsupervised learning models that encode graph data into a latent space and then decode it back.\n",
    "\n",
    "**Graph-Based Algorithms**:\n",
    "    \n",
    "    * PageRank: An algorithm used to rank nodes in a graph based on their importance.\n",
    "    * Label Propagation: A semi-supervised learning algorithm that propagates labels through the graph.\n",
    "    * Community Detection: Algorithms like Louvain and Girvan-Newman that identify clusters or communities within a graph.\n",
    "\n",
    "**Graph Generative Models**\n",
    "    \n",
    "    * GraphRNN: A recurrent neural network-based model for generating graphs.\n",
    "    * GraphVAE: A variational autoencoder for generating graphs.\n",
    "    * GraphGAN: A generative adversarial network for generating graphs.\n",
    "\n",
    "**Some Applications**\n",
    "\n",
    "    * Social Network Analysis: Analyzing social networks to understand relationships and influence.\n",
    "    * Recommendation Systems: Using graph-based techniques to recommend items or connections.\n",
    "    * Molecular Graphs: Analyzing molecular structures for drug discovery and chemistry.\n",
    "    * Knowledge Graphs: Representing and reasoning over knowledge in a structured form.\n",
    "\n",
    "These are just a few examples of the diverse techniques and applications in graph-based ML. We can find more details in the [Hugging Face blog](https://huggingface.co/blog/intro-graphml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'Saved_ML_models_Exp1' already exists at c:\\Users\\rouss\\Documents\\GitHub\\Many_MLs_for_Node_Classification\\Saved_ML_models_Exp1\n"
     ]
    }
   ],
   "source": [
    "# First thing, get some essential Packages\n",
    "# We also create a new directory to save the models\n",
    "\n",
    "# Numpy for matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "\n",
    "# Visualization\n",
    "import networkx as nx\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the name of the directory to be created\n",
    "directory_name = \"Saved_ML_models_Exp1\"\n",
    "\n",
    "# Get the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "# Create the full path for the new directory\n",
    "new_directory_path = os.path.join(current_working_directory, directory_name)\n",
    "\n",
    "# Check if the directory exists, and create it if it does not\n",
    "if not os.path.exists(new_directory_path):\n",
    "    os.makedirs(new_directory_path)\n",
    "    print(f\"Directory '{directory_name}' created at {new_directory_path}\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_name}' already exists at {new_directory_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the CiteSeer Dataset\n",
    "This dataset is available through PyTorch Geometric, a package dedicated to Graph NNs. The CiteSeer is one of the several datasets available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# Import dataset from PyTorch Geometric\n",
    "dataset = Planetoid(root=\".\", name=\"CiteSeer\")\n",
    "\n",
    "data = dataset[0] # We extract the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: CiteSeer()\n",
      "Input Text Data shape: torch.Size([3327, 3703])\n",
      "First five rows of the text data:\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Print information about the dataset\n",
    "print(\"Dataset name:\", dataset)\n",
    "print(\"Input Text Data shape:\", data.x.shape)\n",
    "print(\"First five rows of the text data:\\n\", data.x[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the **text data**, the dataset has 3327 documents as rows, made up of 3703 unique words. The documents are represented as one-hot vectors of length 3703. One hot vectors simply mean that if a word exists, then we assign it's magnitude to be 1 and if not, then we assign the magnitude to be 0. We just to need to follow the same order of words for each document, and that is it.\n",
    "\n",
    "What about the **graph information** provided in the Dataset? The edge information can be accessed as show below and is presented in Coordinate list format. Basically, we are given the coordinates of the nodes connected to each other, separated in two lists of nodes (e.g., 628 to 0, 158 to 1, 486 to 1 and so on). As we see, there are 9104 edges between the 3327 documents we have in the dataset.\n",
    "\n",
    "There are different ways to represent the same information, such as using Adjacency matrices or Adjacency lists. To me, the adjacency matrix is the easiest to internalize (see example below), in which the row and the column indices are the coordinates of nodes and the matrix contains just the weights for each pair of nodes.\n",
    "\n",
    "Now, we know a lot about the data we have. Time to just build the GNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges as coordinate lists:\n",
      "tensor([[ 628,  158,  486,  ..., 2820, 1643,   33],\n",
      "        [   0,    1,    1,  ..., 3324, 3325, 3326]])\n",
      "torch.Size([2, 9104])\n",
      "\n",
      "Adjacency Matrix:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Edges as coordinate lists:\")\n",
    "print(data.edge_index)\n",
    "print(data.edge_index.shape)\n",
    "\n",
    "# Create Adjacency matrix\n",
    "coo_list = data.edge_index\n",
    "\n",
    "adj_matrix = np.zeros([data.x.shape[0], data.x.shape[0]])\n",
    "for (i,j) in zip(coo_list[0,:], coo_list[1,:]): adj_matrix[i, j] = 1\n",
    "\n",
    "print(\"\\nAdjacency Matrix:\\n\", adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that PyTorch is already installed\n",
    "import torch\n",
    "torchversion = torch.__version__\n",
    "\n",
    "# Install PyTorch Scatter, PyTorch Sparse, and PyTorch Geometric\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Dropout\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "\n",
    "from torch_geometric.utils import remove_isolated_nodes\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.utils import degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3a: Building a Graph Convolutional Network\n",
    "\n",
    "First, we need to build the network. The below class implements it and is written in pretty much standard ways of building a PyTorch model. The first function in the class defines the layers we want in the model and the number of inputs and outputs for each layer. The second one defines the transformations and passing of the information from one layer to another, thus completing the network architecture.\n",
    "\n",
    "The beauty of our graph-based ML model lies in the \"GCNConv\" layers from PyTorch. Similar to convolutions in CNNs, GCNs combine information for a node from its neighbors in attempt to find helpful, complex relationships within the data. To do sc, it combines the node features (i.e., the words in our documents) with the information of the edges between nodes. The rest of the layers and also the architecture are similar to the standard Deep Learning counterparts.\n",
    "\n",
    "A last point to note is the input variables to each function. For the init function, we need to provide the input data dimension (i.e., the number of words or features), the number of nodes in the hidden layer (our choice!), and the output dimension (i.e., the number of classes to predict). For the forward function, we provide both the text data (in \"x\") and the edge info (in \"edge_index\") to our model. The text data is presented in one-hot encoded form and the edge info is presented as a co-ordinate list, just as we have in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "  def __init__(self, dim_in, dim_h, dim_out):\n",
    "    super().__init__()\n",
    "    self.gcn1 = GCNConv(dim_in, dim_h)\n",
    "    self.gcn2 = GCNConv(dim_h, dim_out)\n",
    "    self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                      lr=0.01,\n",
    "                                      weight_decay=5e-4)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h = F.dropout(x, p=0.5, training=self.training) # Note: We are using dropouts\n",
    "    h = self.gcn1(h, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.5, training=self.training)\n",
    "    h = self.gcn2(h, edge_index)\n",
    "    return h, F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up our (1) accuracy, (2) train, and (3) test/evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, y):\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def train(model, data):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = model.optimizer\n",
    "    epochs = 200\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        # Training\n",
    "        optimizer.zero_grad()\n",
    "        _, out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        acc = accuracy(out[data.train_mask].argmax(dim = 1), data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "        val_acc = accuracy(out[data.val_mask].argmax(dim = 1), data.y[data.val_mask])\n",
    "\n",
    "        # Print metrics every 10 epochs\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: '\n",
    "                  f'{acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
    "                  f'Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "    return(model)\n",
    "\n",
    "torch.no_grad() # Because we are testing, we do not need gradient calculation\n",
    "def evaluation(model, data, data_mask_list = [\"train_mask\", \"val_mask\", \"test_mask\"]):\n",
    "    model.eval()\n",
    "\n",
    "    # We get the predictions out for ALL DOCUMENTS first!\n",
    "    _, out_pred_labels = model(data.x, data.edge_index)\n",
    "\n",
    "    # Evaluate by train, val, and test sets\n",
    "\n",
    "    list_of_acc_train_val_test = [] # to save the accuracies for different sets\n",
    "    for mask in data_mask_list:\n",
    "        acc = accuracy(out_pred_labels[data[mask]].argmax(dim = 1), data.y[data[mask]])\n",
    "        acc = acc*100 # convert to percentage\n",
    "        list_of_acc_train_val_test.append(acc)\n",
    "    \n",
    "    return(list_of_acc_train_val_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Vaildate, and Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (gcn1): GCNConv(3703, 32)\n",
      "  (gcn2): GCNConv(32, 6)\n",
      ")\n",
      "Epoch   0 | Train Loss: 1.812 | Train Acc:  12.50% | Val Loss: 1.79 | Val Acc: 17.60%\n",
      "Epoch  10 | Train Loss: 0.235 | Train Acc:  95.00% | Val Loss: 1.17 | Val Acc: 61.20%\n",
      "Epoch  20 | Train Loss: 0.069 | Train Acc:  99.17% | Val Loss: 1.40 | Val Acc: 59.20%\n",
      "Epoch  30 | Train Loss: 0.030 | Train Acc:  98.33% | Val Loss: 1.48 | Val Acc: 61.40%\n",
      "Epoch  40 | Train Loss: 0.018 | Train Acc: 100.00% | Val Loss: 1.58 | Val Acc: 59.60%\n",
      "Epoch  50 | Train Loss: 0.023 | Train Acc: 100.00% | Val Loss: 1.54 | Val Acc: 59.40%\n",
      "Epoch  60 | Train Loss: 0.028 | Train Acc:  99.17% | Val Loss: 1.51 | Val Acc: 59.00%\n",
      "Epoch  70 | Train Loss: 0.025 | Train Acc:  99.17% | Val Loss: 1.34 | Val Acc: 62.20%\n",
      "Epoch  80 | Train Loss: 0.033 | Train Acc:  99.17% | Val Loss: 1.46 | Val Acc: 59.40%\n",
      "Epoch  90 | Train Loss: 0.017 | Train Acc: 100.00% | Val Loss: 1.44 | Val Acc: 62.60%\n",
      "Epoch 100 | Train Loss: 0.024 | Train Acc: 100.00% | Val Loss: 1.44 | Val Acc: 62.40%\n",
      "Epoch 110 | Train Loss: 0.031 | Train Acc: 100.00% | Val Loss: 1.53 | Val Acc: 60.20%\n",
      "Epoch 120 | Train Loss: 0.020 | Train Acc: 100.00% | Val Loss: 1.57 | Val Acc: 60.00%\n",
      "Epoch 130 | Train Loss: 0.048 | Train Acc:  96.67% | Val Loss: 1.62 | Val Acc: 60.80%\n",
      "Epoch 140 | Train Loss: 0.012 | Train Acc: 100.00% | Val Loss: 1.56 | Val Acc: 60.00%\n",
      "Epoch 150 | Train Loss: 0.020 | Train Acc: 100.00% | Val Loss: 1.48 | Val Acc: 61.40%\n",
      "Epoch 160 | Train Loss: 0.036 | Train Acc:  99.17% | Val Loss: 1.54 | Val Acc: 59.00%\n",
      "Epoch 170 | Train Loss: 0.015 | Train Acc: 100.00% | Val Loss: 1.59 | Val Acc: 61.00%\n",
      "Epoch 180 | Train Loss: 0.027 | Train Acc: 100.00% | Val Loss: 1.51 | Val Acc: 63.20%\n",
      "Epoch 190 | Train Loss: 0.018 | Train Acc: 100.00% | Val Loss: 1.48 | Val Acc: 63.60%\n",
      "Epoch 200 | Train Loss: 0.025 | Train Acc:  99.17% | Val Loss: 1.55 | Val Acc: 62.20%\n",
      "\n",
      "--------------------\n",
      "GCN Train-Val-Test Accuracy (in % accuracy): \n",
      " [100.0, 68.00000071525574, 66.69999957084656]\n",
      "CPU times: total: 1.81 s\n",
      "Wall time: 4.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Create an untrained GCN model\n",
    "gcn_model_3a = GCN(dataset.num_features, 32, dataset.num_classes)\n",
    "print(gcn_model_3a)\n",
    "\n",
    "# 2. Train or Fit the model to data\n",
    "train(gcn_model_3a, data)\n",
    "\n",
    "# 3. Check Accuracies on TRAIN-VALIDATION-TEST sets\n",
    "list_acc = evaluation(gcn_model_3a, data)\n",
    "print(\"\\n\" + \"--\"*10)\n",
    "print('GCN Train-Val-Test Accuracy (in % accuracy): \\n', list_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3b: Building a Graph Attention Network\n",
    "\n",
    "The below class implements the GAT. Once again, the first function in the class defines the layers in the model and the number of inputs and outputs for each layer. The second one defines the transformations and message-passing from one layer to another, completing the network.\n",
    "\n",
    "Similar to \"GCNConv\", PyTorch provides \"GATv2Conv\" layers. As the name suggests, GAT uses the powers of convolution but integrates with an \"attention\" mechanism. For this reason, whereas GCNs are closely related to CNNs, GATs are similar to Transformers and its attention network. In fact, Transformers can even be imagined as [a special case of GNNs](https://mlabonne.github.io/blog/posts/2022-03-09-Graph_Attention_Network.html). The GATs use an attention mechanism with the observation that not all nodes are equally important and the amounts of attention to each node can be varied based on their importance to improve computational efficiency and prediction accuracy. We do it by creating \"attention heads\" in charge of attending to the nodes based on their importance. Please see [this tutorial by Maxime Labonne](https://mlabonne.github.io/blog/posts/2022-03-09-Graph_Attention_Network.html) for a visualization of the process and some details about the mathematical transformation involved in the process.\n",
    "\n",
    "To note, this notion of varying importance of nodes is present in many areas of Graph Theory and Graph-based MLs. For example, we could have calculated the node importance as PageRank values and used it as a feature in a traditional machine learning pipeline. However, it is a tricky concept due to the different ways a node could be important, so the choice of importance measures is sophisticated and depends on applications. Fortunately, Graph representation learning and the GNNs both learn the representations directly from data, thus avoiding the difficulties associated with a feature engineering approach.\n",
    "\n",
    "There are many more interesting points to think about in the complex but beautiful world of Graphs. However, here we will focus on implementation first to build a solid foundation to convert our thoughts about relationships in graphs to actions.\n",
    "\n",
    "Again, a last point to note before we start modeling is the input variables to each function. For the init function, we do the same as before -- provide the input data dimension (i.e., the number of words or features), the number of nodes in the hidden layer (our choice!), and the output dimension (i.e., the number of classes to predict) -- but also need to provide the number of attention heads. Higher number of heads will increase computational load and computation time. The way we set up the codes, its value can be set while specifying the models. Here, we experiment between 8 or 16 heads. For the forward function, we do the same as we did with GCNs -- the text data is passed to \"x\" and the edge info to \"edge_index\".\n",
    "\n",
    "<!-- A last point to note is the input variables in the forward function. We provide both the text data (in \"x\") and the edge info (in \"edge_index\") to our model. The text data is presented in one-hot encoded form and the edge info is presented as a co-ordinate list, just as we have in our dataset. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "  def __init__(self, dim_in, dim_h, dim_out, heads=16):\n",
    "    super().__init__()\n",
    "    self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)\n",
    "    self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1)\n",
    "    self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                      lr=0.005,\n",
    "                                      weight_decay=5e-4)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h = F.dropout(x, p=0.5, training=self.training)\n",
    "    h = self.gat1(x, edge_index)\n",
    "    h = F.elu(h)\n",
    "    h = F.dropout(h, p=0.5, training=self.training)\n",
    "    h = self.gat2(h, edge_index)\n",
    "    return h, F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have our accuracy, train, and evaluation functions set up and we can go directly to training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (gat1): GATv2Conv(3703, 32, heads=8)\n",
      "  (gat2): GATv2Conv(256, 6, heads=1)\n",
      ")\n",
      "Epoch   0 | Train Loss: 1.791 | Train Acc:  23.33% | Val Loss: 1.77 | Val Acc: 25.00%\n",
      "Epoch  10 | Train Loss: 0.004 | Train Acc: 100.00% | Val Loss: 1.32 | Val Acc: 63.40%\n",
      "Epoch  20 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 1.72 | Val Acc: 59.80%\n",
      "Epoch  30 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 1.63 | Val Acc: 62.40%\n",
      "Epoch  40 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 1.46 | Val Acc: 63.40%\n",
      "Epoch  50 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 1.31 | Val Acc: 63.80%\n",
      "Epoch  60 | Train Loss: 0.004 | Train Acc: 100.00% | Val Loss: 1.19 | Val Acc: 67.20%\n",
      "Epoch  70 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 1.13 | Val Acc: 67.00%\n",
      "Epoch  80 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 68.00%\n",
      "Epoch  90 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 1.17 | Val Acc: 68.20%\n",
      "Epoch 100 | Train Loss: 0.004 | Train Acc: 100.00% | Val Loss: 1.15 | Val Acc: 68.20%\n",
      "Epoch 110 | Train Loss: 0.004 | Train Acc: 100.00% | Val Loss: 1.13 | Val Acc: 66.00%\n",
      "Epoch 120 | Train Loss: 0.004 | Train Acc: 100.00% | Val Loss: 1.15 | Val Acc: 67.60%\n",
      "Epoch 130 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 1.16 | Val Acc: 68.20%\n",
      "Epoch 140 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 1.15 | Val Acc: 67.60%\n",
      "Epoch 150 | Train Loss: 0.004 | Train Acc: 100.00% | Val Loss: 1.16 | Val Acc: 68.20%\n",
      "Epoch 160 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 1.16 | Val Acc: 67.20%\n",
      "Epoch 170 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 1.17 | Val Acc: 68.60%\n",
      "Epoch 180 | Train Loss: 0.004 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 67.80%\n",
      "Epoch 190 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 1.18 | Val Acc: 67.80%\n",
      "Epoch 200 | Train Loss: 0.004 | Train Acc: 100.00% | Val Loss: 1.16 | Val Acc: 67.60%\n",
      "\n",
      "--------------------\n",
      "GAT Train-Val-Test Accuracy (in % accuracy): \n",
      " [100.0, 67.1999990940094, 66.79999828338623]\n",
      "CPU times: total: 9.48 s\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Create an untrained gat model\n",
    "gat_model_3b = GAT(dataset.num_features, 32, dataset.num_classes, heads = 8)\n",
    "print(gat_model_3b)\n",
    "\n",
    "# 2. Train or Fit the model to data\n",
    "train(gat_model_3b, data)\n",
    "\n",
    "# 3. Check Accuracies on TRAIN-VALIDATION-TEST sets\n",
    "list_acc = evaluation(gat_model_3b, data)\n",
    "print(\"\\n\" + \"--\"*10)\n",
    "print('GAT Train-Val-Test Accuracy (in % accuracy): \\n', list_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the GATs are much slower than GCNs. In our case, the GATs did not improve the accuracy by much over GCNs either. However, GATs have been found to consistently outperform the accuracies of GCNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What comes next?\n",
    "We will try to \"hide\" information from our GCNs and GATs, and see how well do they do when presented with only text or only edge information. Exciting, right?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
