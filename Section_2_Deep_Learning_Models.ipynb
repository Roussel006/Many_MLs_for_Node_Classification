{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Experiments with Deep Learning Models\n",
    "\n",
    "In this section, we will build eight \"shallow\" ML classifiers for text/node classification. We will learn how to implement each of them using Scikit-Learn and save the models for later comparisons with other models. The models we will develop are:\n",
    "\n",
    "    1a. Naive Bayes\n",
    "    1b. XGBoost\n",
    "    1c. Decision Trees\n",
    "    1d. Random Forest\n",
    "    1e. Gradient Boosting\n",
    "    1f. CatBoost\n",
    "    1g. LightGBM\n",
    "    1h. Support Vector Machine (SVM) Classifiers\n",
    "\n",
    "\n",
    "**The Dataset** we will use is the CiteSeer Dataset and classify the documents or the nodes. This dataset is a popular benchmark for Graph-based MLs. As of January 2025, the best accuracy achieved is **82.07 Â± 1.04** by [\"ACMII-Snowball-2\"](https://paperswithcode.com/paper/is-heterophily-a-real-nightmare-for-graph). A live update on the rankings can be found in this [link](https://paperswithcode.com/sota/node-classification-on-citeseer).\n",
    "\n",
    "Can we beat it? Perhaps not so easily, as brilliant ML scientists and engineers have already thrown the kitchen sink at it. But we can definitely try! Why not dream? We will see how close we can get.\n",
    "\n",
    "The information within the dataset: This dataset contains a set of 3327 scientific papers represented by binary vectors of 3703 words, with the values represent the presence or absence of the words in the document. A **key feature** of the dataset is that it also contains data on the citations among the papers as a citation graph or network, along with the text data. Here we are only use the text data. In later sections, we will incorporate the Graph data and see how it changes things. The availability of both types of data is the biggest reason we picked this dataset.\n",
    "\n",
    "**The General Plan**:\n",
    "1. <u>Build a Modeling Pipeline</u>: For each model, we will create a \"pipeline\". The pipelines can include everything between inputs and outputs. For example, we may want to represent our texts as certain kind of vectors (e.g., one_hot, TF-IDF). Then, We may want to transform our vectors and reduce their dimensions using methods such as Singular Value Decomposition (SVD) or Non-negative Matrix Factorization (NMF). Finally, we would have our model to feed these all into. This workflow can be conveniently represented as a pipeline, as we will see.\n",
    "\n",
    "2. <u>Train, Validate, and Test</u>: After training, we will check the validation and the test accuracies. \n",
    "\n",
    "3. <u>Save the Models</u>: We will then save the models so that we can call them up again in later sections.\n",
    "\n",
    "It is almost as simple as it sounds. Of course, there are some nuances to these methods. But, we do not need to worry too much about it now. We will discuss things as they become necessary.\n",
    "\n",
    "Enough talking! Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'Saved_ML_models_Exp1' created at c:\\Users\\rouss\\Documents\\GitHub\\Many_MLs_for_Node_Classification\\Saved_ML_models_Exp1\n"
     ]
    }
   ],
   "source": [
    "# First thing, get some essential Packages\n",
    "# We also create a new directory to save the models\n",
    "\n",
    "# Numpy for matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "\n",
    "# Visualization\n",
    "import networkx as nx\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the name of the directory to be created\n",
    "directory_name = \"Saved_ML_models_Exp1\"\n",
    "\n",
    "# Get the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "# Create the full path for the new directory\n",
    "new_directory_path = os.path.join(current_working_directory, directory_name)\n",
    "\n",
    "# Check if the directory exists, and create it if it does not\n",
    "if not os.path.exists(new_directory_path):\n",
    "    os.makedirs(new_directory_path)\n",
    "    print(f\"Directory '{directory_name}' created at {new_directory_path}\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_name}' already exists at {new_directory_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the CiteSeer Dataset\n",
    "This dataset is available through PyTorch Geometric, a package dedicated to Graph NNs. The CiteSeer is one of the several datasets available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# Import dataset from PyTorch Geometric\n",
    "dataset = Planetoid(root=\".\", name=\"CiteSeer\")\n",
    "\n",
    "data = dataset[0] # We extract the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: CiteSeer()\n",
      "Input Text Data shape: torch.Size([3327, 3703])\n",
      "First five rows of the text data:\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Print information about the dataset\n",
    "print(\"Dataset name:\", dataset)\n",
    "print(\"Input Text Data shape:\", data.x.shape)\n",
    "print(\"First five rows of the text data:\\n\", data.x[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the dataset has 3327 documents as rows, made up of 3703 unique words. The documents are represented as one-hot vectors of length 3703. One hot vectors simply mean that if a word exists, then we assign it's magnitude to be 1 and if not, then we assign the magnitude to be 0. We just to need to follow the same order of words for each document, and that is it.\n",
    "\n",
    "An interesting point is the array type, which is \"torch.tensor\". Torch tensors are perfectly compatible with Numpy, so we should be fine.\n",
    "\n",
    "Now, we are ready to get modeling!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
